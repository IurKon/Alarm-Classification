{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTENC, RandomOverSampler #using this oversampling method, since the original data set is imbalanced\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data (file 'example_data.csv' with a separator ';')\n",
    "#columns in this example: 'sensor role', 'supply chain leg', 'distance to previous generated event (leg_rel)',\n",
    "#'distance to expected next event (leg_abs)', 'atmosphere temperature at current location (temp_p)', 'setpoint deviation (spd)',\n",
    "#'slope of two recent measurements', 'average deviation before a triggered alarm within one hour',\n",
    "#'average deviation after a triggered alarm within one next hour' (estimated with the help of random forest regressor),\n",
    "#'lower threshold', 'higher threshold', 'alarm label' (target feature)\n",
    "df = pd.read_csv('example_data.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescaling ambient temperature in terms of setpoint deviation units\n",
    "#df.iloc[i, 10] corresponds to higher threshold, df.iloc[i, 9] - to lower, and df.iloc[i, 4] contains the initial unscaled ambient temperature value\n",
    "for i in range(len(df)):\n",
    "    middle = (df.iloc[i, 10] - df.iloc[i, 9])/2 + df.iloc[i, 9]\n",
    "    span = (df.iloc[i, 10] - df.iloc[i, 9])/2\n",
    "    if df.iloc[i, 4] > middle:\n",
    "        df.iloc[i, 4] = (df.iloc[i, 4] - middle)/span\n",
    "    elif df.iloc[i, 4] < middle:\n",
    "        df.iloc[i, 4] = (df.iloc[i, 4] - middle)/span\n",
    "    else:\n",
    "        df.iloc[i, 4] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features for tree-based algorithms (do not require normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using only needed columns for predictor features and a target feature\n",
    "y = df['label']\n",
    "X = df[['sen_role', 'sc_leg', 'leg_rel', 'leg_abs', 'temp_p', 'spd', 'slope', 'db_1h', 'da_1h']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding categorical features as dummy features\n",
    "enc = OrdinalEncoder()\n",
    "X.sen_role = enc.fit_transform(X.sen_role.values.reshape(-1, 1))\n",
    "X.sc_leg = enc.fit_transform(X.sc_leg.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features for otheralgorithms (except naive Bayes) that require normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using only needed columns for predictor features and a target feature\n",
    "yo = df['label']\n",
    "Xo = df[['sen_role', 'sc_leg', 'leg_rel', 'leg_abs', 'temp_p', 'spd', 'slope', 'db_1h', 'da_1h']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting categorical variables into dummy variables and creating additional columns for this purpose\n",
    "Xo = pd.get_dummies(Xo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking whether conversion went without errors\n",
    "Xo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting columns that do not contain additional information\n",
    "Xo = Xo.drop(['sen_role_AMB', 'sc_leg_e'], axis = 1) #the exact names may differ depending on the naming convention in a different data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['leg_rel', 'leg_abs', 'temp_p', 'spd', 'slope', 'db_1h', 'da_1h'] #the exact names may differ depending on the naming convention in a different data set\n",
    "for i in range(7):\n",
    "    for j in range(len(Xo)):\n",
    "        minimum = min(Xo[features[i]])\n",
    "        maximum = max(Xo[features[i]])\n",
    "        Xo.iloc[j, i] = (Xo.iloc[j, i] - minimum)/(maximum - minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the results of normalization and categorical encoding as dummy features\n",
    "Xo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for the best hyperparameter combination for each classifier and collecting scores for evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists for collecting scores of evaluation metrics\n",
    "knn_acc = []\n",
    "knn_auc = []\n",
    "knn_pr = []\n",
    "knn_rec = []\n",
    "knn_gin = []\n",
    "\n",
    "#classifier instance initialization\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#creating an instance of oversampler and incorporating it into a pipeline\n",
    "smtnc = SMOTENC(categorical_features = [7, 8, 9, 10], sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "model = Pipeline([('smtnc', smtnc), ('clf', knn)])\n",
    "\n",
    "#specifying a domain of hyperparameter values and running a grid search over them\n",
    "params = {'clf__n_neighbors': [your_range]} #as 'your_range' specify all possible values of k to search among; all values to 100 have been used in the project\n",
    "grid = GridSearchCV(model, params, cv = 10, scoring = 'accuracy', n_jobs = -1)\n",
    "grid.fit(Xo, yo)\n",
    "\n",
    "#extracting the value(s) of the optimal hyperparameter(s)\n",
    "res = grid.best_params_\n",
    "in_par = []\n",
    "for key, value in res.items():\n",
    "    temp = [key, value]\n",
    "    in_par.append(temp)\n",
    "final = {}\n",
    "for i in in_par:\n",
    "    raw_parameter = i[0]\n",
    "    value = i[1]\n",
    "    parameter = raw_parameter[5:]\n",
    "    final[parameter] = value\n",
    "    \n",
    "#new instance of classifier to be executed with the optimal hyperparameter(s)    \n",
    "knn_new = KNeighborsClassifier()\n",
    "knn_new.set_params(**final)\n",
    "model_new = Pipeline([('smtnc', smtnc), ('clf', knn_new)])\n",
    "\n",
    "#collection of average results (evaluation metric values)\n",
    "for j in range(100):\n",
    "    acc = cross_val_score(model_new, Xo, yo, scoring = 'accuracy', cv = 10)\n",
    "    for k in acc:\n",
    "        knn_acc.append(k)\n",
    "    auc = cross_val_score(model_new, Xo, yo, scoring = 'roc_auc', cv = 10)\n",
    "    for k in auc:\n",
    "        knn_auc.append(k)\n",
    "        knn_gin.append(2*k - 1)\n",
    "    pr = cross_val_score(model_new, Xo, yo, scoring = 'precision', cv = 10)\n",
    "    for k in pr:\n",
    "        knn_pr.append(k)\n",
    "    rec = cross_val_score(model_new, Xo, yo, scoring = 'recall', cv = 10)\n",
    "    for k in rec:\n",
    "        knn_rec.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value(s) of the optimal hyperparameter(s)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing scores of evaluation metrics and their standard deviations\n",
    "print('acc (mean): ', sum(knn_acc)/len(knn_acc), 'acc (std): ', np.std(knn_acc),\n",
    "      '\\nauc (mean): ', sum(knn_auc)/len(knn_auc), 'auc (std): ', np.std(knn_auc),\n",
    "      '\\ngin (mean): ', sum(knn_gin)/len(knn_gin), 'gin (std): ', np.std(knn_gin),\n",
    "     '\\npr (mean): ', sum(knn_pr)/len(knn_pr), 'pr (std): ', np.std(knn_pr),\n",
    "     '\\nrec (mean): ', sum(knn_rec)/len(knn_rec), 'rec (std): ', np.std(knn_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to that with k-NN above, therefore no explanations will be given for parts of code\n",
    "rf_acc = []\n",
    "rf_auc = []\n",
    "rf_pr = []\n",
    "rf_rec = []\n",
    "rf_gin = []\n",
    "rf = RandomForestClassifier()\n",
    "smtnc = SMOTENC(categorical_features = [7, 8, 9, 10], sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "model = Pipeline([('smtnc', smtnc), ('clf', rf)])\n",
    "params = {'clf__n_estimators': [your_range],\n",
    "          'clf__max_features': (your_values), 'clf__max_depth': [your_range],\n",
    "          'clf__min_samples_leaf': [your_range]} #refer to the documentation on scikit-learn with regard to all possible hyperparameters; in this project, these hyperparameters have been tuned\n",
    "grid = GridSearchCV(model, params, cv = 10, scoring = 'accuracy', n_jobs = -1)\n",
    "grid.fit(Xo, yo)\n",
    "res = grid.best_params_\n",
    "in_par = []\n",
    "for key, value in res.items():\n",
    "    temp = [key, value]\n",
    "    in_par.append(temp)\n",
    "final = {}\n",
    "for i in in_par:\n",
    "    raw_parameter = i[0]\n",
    "    value = i[1]\n",
    "    parameter = raw_parameter[5:]\n",
    "    final[parameter] = value\n",
    "rf_new = RandomForestClassifier()\n",
    "rf_new.set_params(**final)\n",
    "model_new = Pipeline([('smtnc', smtnc), ('clf', rf_new)])\n",
    "for j in range(100):\n",
    "    acc = cross_val_score(model_new, Xo, yo, scoring = 'accuracy', cv = 10)\n",
    "    for k in acc:\n",
    "        rf_acc.append(k)\n",
    "    auc = cross_val_score(model_new, Xo, yo, scoring = 'roc_auc', cv = 10)\n",
    "    for k in auc:\n",
    "        rf_auc.append(k)\n",
    "        rf_gin.append(2*k - 1)\n",
    "    pr = cross_val_score(model_new, Xo, yo, scoring = 'precision', cv = 10)\n",
    "    for k in pr:\n",
    "        rf_pr.append(k)\n",
    "    rec = cross_val_score(model_new, Xo, yo, scoring = 'recall', cv = 10)\n",
    "    for k in rec:\n",
    "        rf_rec.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value(s) of the optimal hyperparameter(s)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing scores of evaluation metrics and their standard deviations\n",
    "print('acc (mean): ', sum(rf_acc)/len(rf_acc), 'acc (std): ', np.std(rf_acc),\n",
    "      '\\nauc (mean): ', sum(rf_auc)/len(rf_auc), 'auc (std): ', np.std(rf_auc),\n",
    "      '\\ngin (mean): ', sum(rf_gin)/len(rf_gin), 'gin (std): ', np.std(rf_gin),\n",
    "     '\\npr (mean): ', sum(rf_pr)/len(rf_pr), 'pr (std): ', np.std(rf_pr),\n",
    "     '\\nrec (mean): ', sum(rf_rec)/len(rf_rec), 'rec (std): ', np.std(rf_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to that with k-NN above, therefore no explanations will be given for parts of code\n",
    "svm_acc = []\n",
    "svm_auc = []\n",
    "svm_pr = []\n",
    "svm_rec = []\n",
    "svm_gin = []\n",
    "svm = SVC()\n",
    "smtnc = SMOTENC(categorical_features = [7, 8, 9, 10], sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "model = Pipeline([('smtnc', smtnc), ('clf', svm)])\n",
    "params = {'clf__C': [your_range], 'clf__kernel': (your_values),\n",
    "          'clf__gamma': [your_range], 'clf__coef0': [your_range], 'clf__degree': [your_range]} #refer to the documentation on scikit-learn with regard to all possible hyperparameters; in this project, these hyperparameters have been tuned\n",
    "grid = RandomizedSearchCV(model, params, cv = 10, scoring = 'accuracy', n_jobs = -1)\n",
    "grid.fit(Xo, yo)\n",
    "res = grid.best_params_\n",
    "in_par = []\n",
    "for key, value in res.items():\n",
    "    temp = [key, value]\n",
    "    in_par.append(temp)\n",
    "final = {}\n",
    "for i in in_par:\n",
    "    raw_parameter = i[0]\n",
    "    value = i[1]\n",
    "    parameter = raw_parameter[5:]\n",
    "    final[parameter] = value\n",
    "svm_new = SVC()\n",
    "svm_new.set_params(**final)\n",
    "model_new = Pipeline([('smtnc', smtnc), ('clf', svm_new)])\n",
    "for j in range(100):\n",
    "    acc = cross_val_score(model_new, Xo, yo, scoring = 'accuracy', cv = 10)\n",
    "    for k in acc:\n",
    "        svm_acc.append(k)\n",
    "    auc = cross_val_score(model_new, Xo, yo, scoring = 'roc_auc', cv = 10)\n",
    "    for k in auc:\n",
    "        svm_auc.append(k)\n",
    "        svm_gin.append(2*k - 1)\n",
    "    pr = cross_val_score(model_new, Xo, yo, scoring = 'precision', cv = 10)\n",
    "    for k in pr:\n",
    "        svm_pr.append(k)\n",
    "    rec = cross_val_score(model_new, Xo, yo, scoring = 'recall', cv = 10)\n",
    "    for k in rec:\n",
    "        svm_rec.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value(s) of the optimal hyperparameter(s)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing scores of evaluation metrics and their standard deviations\n",
    "print('acc (mean): ', sum(svm_acc)/len(svm_acc), 'acc (std): ', np.std(svm_acc),\n",
    "      '\\nauc (mean): ', sum(svm_auc)/len(svm_auc), 'auc (std): ', np.std(svm_auc),\n",
    "      '\\ngin (mean): ', sum(svm_gin)/len(svm_gin), 'gin (std): ', np.std(svm_gin),\n",
    "     '\\npr (mean): ', sum(svm_pr)/len(svm_pr), 'pr (std): ', np.std(svm_pr),\n",
    "     '\\nrec (mean): ', sum(svm_rec)/len(svm_rec), 'rec (std): ', np.std(svm_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to that with k-NN above, therefore no explanations will be given for parts of code\n",
    "abc_acc = []\n",
    "abc_auc = []\n",
    "abc_pr = []\n",
    "abc_rec = []\n",
    "abc_gin = []\n",
    "abc = AdaBoostClassifier()\n",
    "smtnc = SMOTENC(categorical_features = [7,8,9,10], sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "model = Pipeline([('smtnc', smtnc), ('clf', abc)])\n",
    "params = {'clf__base_estimator': your_estimators_with_hyperparameters),\n",
    "          'clf__n_estimators': [your_range],\n",
    "          'clf__learning_rate': [your_range]} #refer to the documentation on scikit-learn with regard to all possible hyperparameters; in this project, these hyperparameters have been tuned\n",
    "grid = GridSearchCV(model, params, cv = 10, scoring = 'accuracy', n_jobs = -1)\n",
    "grid.fit(Xo, yo)\n",
    "res = grid.best_params_\n",
    "in_par = []\n",
    "for key, value in res.items():\n",
    "    temp = [key, value]\n",
    "    in_par.append(temp)\n",
    "final = {}\n",
    "for i in in_par:\n",
    "    raw_parameter = i[0]\n",
    "    value = i[1]\n",
    "    parameter = raw_parameter[5:]\n",
    "    final[parameter] = value\n",
    "abc_new = AdaBoostClassifier()\n",
    "abc_new.set_params(**final)\n",
    "model_new = Pipeline([('smtnc', smtnc), ('clf', abc_new)])\n",
    "for j in range(100):\n",
    "    acc = cross_val_score(model_new, Xo, yo, scoring = 'accuracy', cv = 10)\n",
    "    for k in acc:\n",
    "        abc_acc.append(k)\n",
    "    auc = cross_val_score(model_new, Xo, yo, scoring = 'roc_auc', cv = 10)\n",
    "    for k in auc:\n",
    "        abc_auc.append(k)\n",
    "        abc_gin.append(2*k - 1)\n",
    "    pr = cross_val_score(model_new, Xo, yo, scoring = 'precision', cv = 10)\n",
    "    for k in pr:\n",
    "        abc_pr.append(k)\n",
    "    rec = cross_val_score(model_new, Xo, yo, scoring = 'recall', cv = 10)\n",
    "    for k in rec:\n",
    "        abc_rec.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value(s) of the optimal hyperparameter(s)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing scores of evaluation metrics and their standard deviations\n",
    "print('acc (mean): ', sum(abc_acc)/len(abc_acc), 'acc (std): ', np.std(abc_acc),\n",
    "      '\\nauc (mean): ', sum(abc_auc)/len(abc_auc), 'auc (std): ', np.std(abc_auc),\n",
    "      '\\ngin (mean): ', sum(abc_gin)/len(abc_gin), 'gin (std): ', np.std(abc_gin),\n",
    "     '\\npr (mean): ', sum(abc_pr)/len(abc_pr), 'pr (std): ', np.std(abc_pr),\n",
    "     '\\nrec (mean): ', sum(abc_rec)/len(abc_rec), 'rec (std): ', np.std(abc_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to that with k-NN above, therefore no explanations will be given for parts of code\n",
    "mlp_acc = []\n",
    "mlp_auc = []\n",
    "mlp_pr = []\n",
    "mlp_rec = []\n",
    "mlp_gin = []\n",
    "mlp = MLPClassifier()\n",
    "smtnc = SMOTENC(categorical_features = [7,8,9,10], sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "model = Pipeline([('smtnc', smtnc), ('clf', mlp)])\n",
    "params = {'clf__hidden_layer_sizes': (your_architectures),\n",
    "          'clf__activation': (your_values), 'solver': (your_values),\n",
    "          'clf__alpha': [your_range],\n",
    "          'clf__learning_rate': (your_values),\n",
    "         'clf__learning_rate_init': [your_range]} #refer to the documentation on scikit-learn with regard to all possible hyperparameters; in this project, these hyperparameters have been tuned\n",
    "grid = GridSearchCV(model, params, cv = 10, scoring = 'accuracy', n_jobs = -1)\n",
    "grid.fit(Xo, yo)\n",
    "res = grid.best_params_\n",
    "in_par = []\n",
    "for key, value in res.items():\n",
    "    temp = [key, value]\n",
    "    in_par.append(temp)\n",
    "final = {}\n",
    "for i in in_par:\n",
    "    raw_parameter = i[0]\n",
    "    value = i[1]\n",
    "    parameter = raw_parameter[5:]\n",
    "    final[parameter] = value\n",
    "mlp_new = MLPClassifier()\n",
    "mlp_new.set_params(**final)\n",
    "model_new = Pipeline([('smtnc', smtnc), ('clf', mlp_new)])\n",
    "for j in range(100):\n",
    "    acc = cross_val_score(model_new, Xo, yo, scoring = 'accuracy', cv = 10)\n",
    "    for k in acc:\n",
    "        mlp_acc.append(k)\n",
    "    auc = cross_val_score(model_new, Xo, yo, scoring = 'roc_auc', cv = 10)\n",
    "    for k in auc:\n",
    "        mlp_auc.append(k)\n",
    "        mlp_gin.append(2*k - 1)\n",
    "    pr = cross_val_score(model_new, Xo, yo, scoring = 'precision', cv = 10)\n",
    "    for k in pr:\n",
    "        mlp_pr.append(k)\n",
    "    rec = cross_val_score(model_new, Xo, yo, scoring = 'recall', cv = 10)\n",
    "    for k in rec:\n",
    "        mlp_rec.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value(s) of the optimal hyperparameter(s)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing scores of evaluation metrics and their standard deviations\n",
    "print('acc (mean): ', sum(mlp_acc)/len(mlp_acc), 'acc (std): ', np.std(mlp_acc),\n",
    "      '\\nauc (mean): ', sum(mlp_auc)/len(mlp_auc), 'auc (std): ', np.std(mlp_auc),\n",
    "      '\\ngin (mean): ', sum(mlp_gin)/len(mlp_gin), 'gin (std): ', np.std(mlp_gin),\n",
    "     '\\npr (mean): ', sum(mlp_pr)/len(mlp_pr), 'pr (std): ', np.std(mlp_pr),\n",
    "     '\\nrec (mean): ', sum(mlp_rec)/len(mlp_rec), 'rec (std): ', np.std(mlp_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to that with k-NN above, therefore no explanations will be given for parts of code\n",
    "lr_acc = []\n",
    "lr_auc = []\n",
    "lr_pr = []\n",
    "lr_rec = []\n",
    "lr_gin = []\n",
    "lr = LogisticRegression()\n",
    "smtnc = SMOTENC(categorical_features = [7,8,9,10], sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "model = Pipeline([('smtnc', smtnc), ('clf', lr)])\n",
    "params = {'clf__penalty': (your_values),\n",
    "          'clf__C': [your_range],\n",
    "          'clf__max_iter': [your_values]} #refer to the documentation on scikit-learn with regard to all possible hyperparameters; in this project, these hyperparameters have been tuned\n",
    "grid = GridSearchCV(model, params, cv = 10, scoring = 'accuracy', n_jobs = -1)\n",
    "grid.fit(Xo, yo)\n",
    "res = grid.best_params_\n",
    "in_par = []\n",
    "for key, value in res.items():\n",
    "    temp = [key, value]\n",
    "    in_par.append(temp)\n",
    "final = {}\n",
    "for i in in_par:\n",
    "    raw_parameter = i[0]\n",
    "    value = i[1]\n",
    "    parameter = raw_parameter[5:]\n",
    "    final[parameter] = value\n",
    "lr_new = LogisticRegression()\n",
    "lr_new.set_params(**final)\n",
    "model_new = Pipeline([('smtnc', smtnc), ('clf', lr_new)])\n",
    "for j in range(100):\n",
    "    acc = cross_val_score(model_new, Xo, yo, scoring = 'accuracy', cv = 10)\n",
    "    for k in acc:\n",
    "        lr_acc.append(k)\n",
    "    auc = cross_val_score(model_new, Xo, yo, scoring = 'roc_auc', cv = 10)\n",
    "    for k in auc:\n",
    "        lr_auc.append(k)\n",
    "        lr_gin.append(2*k - 1)\n",
    "    pr = cross_val_score(model_new, Xo, yo, scoring = 'precision', cv = 10)\n",
    "    for k in pr:\n",
    "        lr_pr.append(k)\n",
    "    rec = cross_val_score(model_new, Xo, yo, scoring = 'recall', cv = 10)\n",
    "    for k in rec:\n",
    "        lr_rec.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value(s) of the optimal hyperparameter(s)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing scores of evaluation metrics and their standard deviations\n",
    "print('acc (mean): ', sum(lr_acc)/len(lr_acc), 'acc (std): ', np.std(lr_acc),\n",
    "      '\\nauc (mean): ', sum(lr_auc)/len(lr_auc), 'auc (std): ', np.std(lr_auc),\n",
    "      '\\ngin (mean): ', sum(lr_gin)/len(lr_gin), 'gin (std): ', np.std(lr_gin),\n",
    "     '\\npr (mean): ', sum(lr_pr)/len(lr_pr), 'pr (std): ', np.std(lr_pr),\n",
    "     '\\nrec (mean): ', sum(lr_rec)/len(lr_rec), 'rec (std): ', np.std(lr_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to that with k-NN above, therefore no explanations will be given for parts of code\n",
    "dt_acc = []\n",
    "dt_auc = []\n",
    "dt_pr = []\n",
    "dt_rec = []\n",
    "dt_gin = []\n",
    "dt = DecisionTreeClassifier()\n",
    "smtnc = SMOTENC(categorical_features = [7,8,9,10], sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "model = Pipeline([('smtnc', smtnc), ('clf', dt)])\n",
    "params = {'clf__splitter': (your_values), 'clf__max_depth': [your_range],\n",
    "          'clf__min_samples_split': [your_range],\n",
    "          'clf__max_features': (your_values)} #refer to the documentation on scikit-learn with regard to all possible hyperparameters; in this project, these hyperparameters have been tuned\n",
    "grid = GridSearchCV(model, params, cv = 10, scoring = 'accuracy', n_jobs = -1)\n",
    "grid.fit(Xo, yo)\n",
    "res = grid.best_params_\n",
    "in_par = []\n",
    "for key, value in res.items():\n",
    "    temp = [key, value]\n",
    "    in_par.append(temp)\n",
    "final = {}\n",
    "for i in in_par:\n",
    "    raw_parameter = i[0]\n",
    "    value = i[1]\n",
    "    parameter = raw_parameter[5:]\n",
    "    final[parameter] = value\n",
    "dt_new = DecisionTreeClassifier()\n",
    "dt_new.set_params(**final)\n",
    "model_new = Pipeline([('smtnc', smtnc), ('clf', dt_new)])\n",
    "for j in range(100):\n",
    "    acc = cross_val_score(model_new, Xo, yo, scoring = 'accuracy', cv = 10)\n",
    "    for k in acc:\n",
    "        dt_acc.append(k)\n",
    "    auc = cross_val_score(model_new, Xo, yo, scoring = 'roc_auc', cv = 10)\n",
    "    for k in auc:\n",
    "        dt_auc.append(k)\n",
    "        dt_gin.append(2*k - 1)\n",
    "    pr = cross_val_score(model_new, Xo, yo, scoring = 'precision', cv = 10)\n",
    "    for k in pr:\n",
    "        dt_pr.append(k)\n",
    "    rec = cross_val_score(model_new, Xo, yo, scoring = 'recall', cv = 10)\n",
    "    for k in rec:\n",
    "        dt_rec.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value(s) of the optimal hyperparameter(s)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing scores of evaluation metrics and their standard deviations\n",
    "print('acc (mean): ', sum(dt_acc)/len(dt_acc), 'acc (std): ', np.std(dt_acc),\n",
    "      '\\nauc (mean): ', sum(dt_auc)/len(dt_auc), 'auc (std): ', np.std(dt_auc),\n",
    "      '\\ngin (mean): ', sum(dt_gin)/len(dt_gin), 'gin (std): ', np.std(dt_gin),\n",
    "     '\\npr (mean): ', sum(dt_pr)/len(dt_pr), 'pr (std): ', np.std(dt_pr),\n",
    "     '\\nrec (mean): ', sum(dt_rec)/len(dt_rec), 'rec (std): ', np.std(dt_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to that with k-NN above, therefore no explanations will be given for parts of code\n",
    "bag_acc = []\n",
    "bag_auc = []\n",
    "bag_pr = []\n",
    "bag_rec = []\n",
    "bag_gin = []\n",
    "bag = BaggingClassifier()\n",
    "smtnc = SMOTENC(categorical_features = [7,8,9,10], sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "model = Pipeline([('smtnc', smtnc), ('clf', bag)])\n",
    "params = {'clf__n_estimators': [your_range],\n",
    "          'clf__max_samples': [your_range],\n",
    "          'clf__max_features': [your_range]} #refer to the documentation on scikit-learn with regard to all possible hyperparameters; in this project, these hyperparameters have been tuned\n",
    "grid = GridSearchCV(model, params, cv = 10, scoring = 'accuracy', n_jobs = -1)\n",
    "grid.fit(Xo, yo)\n",
    "res = grid.best_params_\n",
    "in_par = []\n",
    "for key, value in res.items():\n",
    "    temp = [key, value]\n",
    "    in_par.append(temp)\n",
    "final = {}\n",
    "for i in in_par:\n",
    "    raw_parameter = i[0]\n",
    "    value = i[1]\n",
    "    parameter = raw_parameter[5:]\n",
    "    final[parameter] = value\n",
    "bag_new = BaggingClassifier()\n",
    "bag_new.set_params(**final)\n",
    "model_new = Pipeline([('smtnc', smtnc), ('clf', bag_new)])\n",
    "for j in range(100):\n",
    "    acc = cross_val_score(model_new, Xo, yo, scoring = 'accuracy', cv = 10)\n",
    "    for k in acc:\n",
    "        bag_acc.append(k)\n",
    "    auc = cross_val_score(model_new, Xo, yo, scoring = 'roc_auc', cv = 10)\n",
    "    for k in auc:\n",
    "        bag_auc.append(k)\n",
    "        bag_gin.append(2*k - 1)\n",
    "    pr = cross_val_score(model_new, Xo, yo, scoring = 'precision', cv = 10)\n",
    "    for k in pr:\n",
    "        bag_pr.append(k)\n",
    "    rec = cross_val_score(model_new, Xo, yo, scoring = 'recall', cv = 10)\n",
    "    for k in rec:\n",
    "        bag_rec.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value(s) of the optimal hyperparameter(s)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing scores of evaluation metrics and their standard deviations\n",
    "print('acc (mean): ', sum(bag_acc)/len(bag_acc), 'acc (std): ', np.std(bag_acc),\n",
    "      '\\nauc (mean): ', sum(bag_auc)/len(bag_auc), 'auc (std): ', np.std(bag_auc),\n",
    "      '\\ngin (mean): ', sum(bag_gin)/len(bag_gin), 'gin (std): ', np.std(bag_gin),\n",
    "     '\\npr (mean): ', sum(bag_pr)/len(bag_pr), 'pr (std): ', np.std(bag_pr),\n",
    "     '\\nrec (mean): ', sum(bag_rec)/len(bag_rec), 'rec (std): ', np.std(bag_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to that with k-NN above, therefore no explanations will be given for parts of code\n",
    "gb_acc = []\n",
    "gb_auc = []\n",
    "gb_pr = []\n",
    "gb_rec = []\n",
    "gb_gin = []\n",
    "gb = GradientBoostingClassifier()\n",
    "smtnc = SMOTENC(categorical_features = [7,8,9,10], sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "model = Pipeline([('smtnc', smtnc), ('clf', gb)])\n",
    "params = {'clf__loss': (your_values), 'clf__learning_rate': [your_range],\n",
    "         'clf__n_estimators': [your_range],\n",
    "         'clf__max_depth': [your_range],\n",
    "          'clf__min_samples_split': [your_range]} #refer to the documentation on scikit-learn with regard to all possible hyperparameters; in this project, these hyperparameters have been tuned\n",
    "grid = GridSearchCV(model, params, cv = 10, scoring = 'accuracy', n_jobs = -1)\n",
    "grid.fit(Xo, yo)\n",
    "res = grid.best_params_\n",
    "in_par = []\n",
    "for key, value in res.items():\n",
    "    temp = [key, value]\n",
    "    in_par.append(temp)\n",
    "final = {}\n",
    "for i in in_par:\n",
    "    raw_parameter = i[0]\n",
    "    value = i[1]\n",
    "    parameter = raw_parameter[5:]\n",
    "    final[parameter] = value\n",
    "gb_new = GradientBoostingClassifier()\n",
    "gb_new.set_params(**final)\n",
    "model_new = Pipeline([('smtnc', smtnc), ('clf', gb_new)])\n",
    "for j in range(100):\n",
    "    acc = cross_val_score(model_new, Xo, yo, scoring = 'accuracy', cv = 10)\n",
    "    for k in acc:\n",
    "        gb_acc.append(k)\n",
    "    auc = cross_val_score(model_new, Xo, yo, scoring = 'roc_auc', cv = 10)\n",
    "    for k in auc:\n",
    "        gb_auc.append(k)\n",
    "        gb_gin.append(2*k - 1)\n",
    "    pr = cross_val_score(model_new, Xo, yo, scoring = 'precision', cv = 10)\n",
    "    for k in pr:\n",
    "        gb_pr.append(k)\n",
    "    rec = cross_val_score(model_new, Xo, yo, scoring = 'recall', cv = 10)\n",
    "    for k in rec:\n",
    "        gb_rec.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#value(s) of the optimal hyperparameter(s)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing scores of evaluation metrics and their standard deviations\n",
    "print('acc (mean): ', sum(gb_acc)/len(gb_acc), 'acc (std): ', np.std(gb_acc),\n",
    "      '\\nauc (mean): ', sum(gb_auc)/len(gb_auc), 'auc (std): ', np.std(gb_auc),\n",
    "      '\\ngin (mean): ', sum(gb_gin)/len(gb_gin), 'gin (std): ', np.std(gb_gin),\n",
    "     '\\npr (mean): ', sum(gb_pr)/len(gb_pr), 'pr (std): ', np.std(gb_pr),\n",
    "     '\\nrec (mean): ', sum(gb_rec)/len(gb_rec), 'rec (std): ', np.std(gb_rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data (file 'example_data.csv' with a separator ',')\n",
    "#columns in this example: 'sensor role', 'supply chain leg', 'distance to previous generated event (leg_rel)',\n",
    "#'distance to expected next event (leg_abs)', 'atmosphere temperature at current location (temp_p)', 'setpoint deviation (spd)',\n",
    "#'slope of two recent measurements', 'average deviation before a triggered alarm within one hour',\n",
    "#'average deviation after a triggered alarm within one next hour' (estimated with the help of random forest regressor),\n",
    "#'alarm label' (target feature)\n",
    "#!!! However, the continuous features for naive Bayes should be discretized with multi-interval discretization method by Fayyad and Irani (1993)\n",
    "dfd = pd.read_csv('example_data.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using only needed columns for predictor features and a target feature\n",
    "yonb = dfd['label']\n",
    "Xonb = dfd[['sen_role', 'sc_leg', 'leg_rel', 'leg_abs', 'temp_p', 'spd', 'slope', 'db_1h', 'da_1h']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting categorical variables into dummy variables and creating additional columns for this purpose\n",
    "Xonb = pd.get_dummies(Xonb)\n",
    "Xonb.columns #printing all resulting columns (to be used for deletion of superfluous dummy features and\n",
    "#declaration of lists 'featurs' and 'remaining_features' in the next steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting columns that do not contain additional information (i.e., one of the columns representing each feature;\n",
    "#it means that n dummy features should be deleted for n initial fatures)\n",
    "Xonb = Xonb.drop([columns_to_drop_separated_by_comma], axis = 1) #instead of 'columns_to_drop_separated_by_comma' specify what columns should be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to that with k-NN above, therefore no explanations will be given for parts of code\n",
    "bnb_acc = []\n",
    "bnb_auc = []\n",
    "bnb_pr = []\n",
    "bnb_rec = []\n",
    "bnb_gin = []\n",
    "bnb = BernoulliNB(parameters) #parameters may be skipped if no additional assumptions are made\n",
    "smtnc = RandomOverSampler(sampling_strategy = 0.5, random_state = 0)\n",
    "model = Pipeline([('smtnc', smtnc), ('clf', bnb)])\n",
    "for j in range(100):\n",
    "    acc = cross_val_score(bnb, Xonb, yonb, scoring = 'accuracy', cv = 10)\n",
    "    for k in acc:\n",
    "        bnb_acc.append(k)\n",
    "    auc = cross_val_score(model, Xonb, yonb, scoring = 'roc_auc', cv = 10)\n",
    "    for k in auc:\n",
    "        bnb_auc.append(k)\n",
    "        bnb_gin.append(2*k - 1)\n",
    "    pr = cross_val_score(model, Xonb, yonb, scoring = 'precision', cv = 10)\n",
    "    for k in pr:\n",
    "        bnb_pr.append(k)\n",
    "    rec = cross_val_score(model, Xonb, yonb, scoring = 'recall', cv = 10)\n",
    "    for k in rec:\n",
    "        bnb_rec.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing scores of evaluation metrics and their standard deviations\n",
    "print('acc (mean): ', sum(bnb_acc)/len(bnb_acc), 'acc (std): ', np.std(bnb_acc),\n",
    "      '\\nauc (mean): ', sum(bnb_auc)/len(bnb_auc), 'auc (std): ', np.std(bnb_auc),\n",
    "      '\\ngin (mean): ', sum(bnb_gin)/len(bnb_gin), 'gin (std): ', np.std(bnb_gin),\n",
    "     '\\npr (mean): ', sum(bnb_pr)/len(bnb_pr), 'pr (std): ', np.std(bnb_pr),\n",
    "     '\\nrec (mean): ', sum(bnb_rec)/len(bnb_rec), 'rec (std): ', np.std(bnb_rec))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
