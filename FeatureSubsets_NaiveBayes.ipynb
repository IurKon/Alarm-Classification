{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, cross_val_predict\n",
    "import itertools\n",
    "import operator\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler #using this oversampling method, since the original data set is imbalanced\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data (file 'example_data.csv' with a separator ';')\n",
    "#columns in this example: 'sensor role', 'supply chain leg', 'distance to previous generated event (leg_rel)',\n",
    "#'distance to expected next event (leg_abs)', 'atmosphere temperature at current location (temp_p)', 'setpoint deviation (spd)',\n",
    "#'slope of two recent measurements', 'average deviation before a triggered alarm within one hour',\n",
    "#'average deviation after a triggered alarm within one next hour' (estimated with the help of random forest regressor),\n",
    "#'alarm label' (target feature)\n",
    "#!!! However, the continuous features for naive Bayes should be discretized with multi-interval discretization method by Fayyad and Irani (1993)\n",
    "df = pd.read_csv('example_data.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using only needed columns for predictor features and a target feature\n",
    "y = df['label']\n",
    "X = df[['sen_role', 'sc_leg', 'leg_rel', 'leg_abs', 'temp_p', 'spd', 'slope', 'db_1h', 'da_1h']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting categorical variables into dummy variables and creating additional columns for this purpose\n",
    "X = pd.get_dummies(X)\n",
    "X.columns #printing all resulting columns (to be used for deletion of superfluous dummy features and\n",
    "#declaration of lists 'featurs' and 'remaining_features' in the next steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting columns that do not contain additional information (i.e., one of the columns representing each feature;\n",
    "#it means that n dummy features should be deleted for n initial fatures)\n",
    "X = X.drop([columns_to_drop_separated_by_comma], axis = 1) #instead of 'columns_to_drop_separated_by_comma' specify what columns should be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying predictor features from which feature (sub)sets will be built and compared\n",
    "features = [] #the exact listing of features will depend on the number of multiple intervals found with the method Fayyad and Irani (1993); should contain all predictor features after deletion of superfluous dummy features in the previous step\n",
    "#!!! features represented by multiple dummy features should be specified in the form 'list of lists', i.e., [[dummy1_feature1, dummy2_feature1, ..., dummyN_feature1], ..., [dummy1_featureN, ..., dummyN_featureN]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loops = loops #instead of 'loops' specify the value for a number of evaluations based on which comparison of scores will undertaken\n",
    "#(the higher the value, the more reliable the comparison; minimum recommended value: 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature (sub)set with seven strongest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = list(itertools.combinations(features, 7)) #iterator for feature combinations\n",
    "\n",
    "#creating dictionaries for different evaluation metrics that will collect feature (sub)sets and their scores\n",
    "#evaluation metrics include accuracy (ACC), area under curve (AUC), Gini coefficient (GIN), precision (PR), recall (REC) and their standard deviations\n",
    "ACC7 = {}\n",
    "AUC7 = {}\n",
    "GIN7 = {}\n",
    "PR7 = {}\n",
    "REC7 = {}\n",
    "ACCstd7 = {}\n",
    "AUCstd7 = {}\n",
    "GINstd7 = {}\n",
    "PRstd7 = {}\n",
    "RECstd7 = {}\n",
    "\n",
    "learner = BernoulliNB(parameters) #parameters may be skipped if no additional assumptions are made\n",
    "\n",
    "smtnc = RandomOverSampler(sampling_strategy = 0.5, random_state = 0) #creating an instance of an oversampler (using this method because predictor features are represented only by categorical data)\n",
    "\n",
    "#loops for each feature (sub)set (irrespective of the order of features in a (sub)set)\n",
    "for i in range(len(comb)):\n",
    "    acc_7 = []\n",
    "    auc_7 = []\n",
    "    pr_7 = []\n",
    "    rec_7 = []\n",
    "    gin_7 = []\n",
    "    cols = []\n",
    "    for j in range(len(comb[i])):\n",
    "        for o in range(len(comb[i][j])):\n",
    "            cols.append(comb[i][j][o])\n",
    "    X_new = X[cols]\n",
    "    model = Pipeline([('smtnc', smtnc), ('clf', learner)])\n",
    "    \n",
    "    #repeating evaluations 'loops' number of times for a comparison later\n",
    "    for l in range(loops):\n",
    "        acc = cross_val_score(model, X_new, y, scoring = 'accuracy', cv = 10)\n",
    "        for m in acc:\n",
    "            acc_7.append(m)\n",
    "        auc = cross_val_score(model, X_new, y, scoring = 'roc_auc', cv = 10)\n",
    "        for m in auc:\n",
    "            auc_7.append(m)\n",
    "            gin_7.append(2*m - 1)\n",
    "        pr = cross_val_score(model, X_new, y, scoring = 'precision', cv = 10)\n",
    "        for m in pr:\n",
    "            pr_7.append(m)\n",
    "        rec = cross_val_score(model, X_new, y, scoring = 'recall', cv = 10)\n",
    "        for m in rec:\n",
    "            rec_7.append(m)\n",
    "            \n",
    "    #appening each of the dictionaries (key: name of a feature (sub)set, value: average evaluation metric value)\n",
    "    ACC7[str(comb[i])] = sum(acc_7)/len(acc_7)\n",
    "    ACCstd7[str(comb[i])] = np.std(acc_7)\n",
    "    AUC7[str(comb[i])] = sum(auc_7)/len(auc_7)\n",
    "    AUCstd7[str(comb[i])] = np.std(auc_7)\n",
    "    GIN7[str(comb[i])] = sum(gin_7)/len(gin_7)\n",
    "    GINstd7[str(comb[i])] = np.std(gin_7)\n",
    "    PR7[str(comb[i])] = sum(pr_7)/len(pr_7)\n",
    "    PRstd7[str(comb[i])] = np.std(pr_7)\n",
    "    REC7[str(comb[i])] = sum(rec_7)/len(rec_7)\n",
    "    RECstd7[str(comb[i])] = np.std(rec_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after evaluation has been completed, user can take a look into each dictionary and make a decision, given multiple evaluation metrics, what feature (sub)set(s) perfromed best\n",
    "#apart from competing cases, in which all (or most) evaluation metrics from one feature (sub)set are not consistently stronger than from another, the following tests should be performed for separate evaluation metrics\n",
    "#non-parametric Kruskal-Wallis test (scipy.stats.mstats.kruskalwallis or scipy.stats.kruskal) and non-parametric pairwise Mann-Whitney U test (scipy.stats.mannwhitneyu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature (sub)set with six strongest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to the search of a feature (sub)set containing seven features, so different parts of code will not be commented\n",
    "comb = list(itertools.combinations(features, 6))\n",
    "ACC6 = {}\n",
    "AUC6 = {}\n",
    "GIN6 = {}\n",
    "PR6 = {}\n",
    "REC6 = {}\n",
    "ACCstd6 = {}\n",
    "AUCstd6 = {}\n",
    "GINstd6 = {}\n",
    "PRstd6 = {}\n",
    "RECstd6 = {}\n",
    "learner = BernoulliNB(parameters)\n",
    "smtnc = RandomOverSampler(sampling_strategy = 0.5, random_state = 0)\n",
    "for i in range(len(comb)):\n",
    "    acc_6 = []\n",
    "    auc_6 = []\n",
    "    pr_6 = []\n",
    "    rec_6 = []\n",
    "    gin_6 = []\n",
    "    cols = []\n",
    "    for j in range(len(comb[i])):\n",
    "        for o in range(len(comb[i][j])):\n",
    "            cols.append(comb[i][j][o])\n",
    "    X_new = X[cols]\n",
    "    model = Pipeline([('smtnc', smtnc), ('clf', learner)])\n",
    "    for l in range(loops):\n",
    "        acc = cross_val_score(model, X_new, y, scoring = 'accuracy', cv = 10)\n",
    "        for m in acc:\n",
    "            acc_6.append(m)\n",
    "        auc = cross_val_score(model, X_new, y, scoring = 'roc_auc', cv = 10)\n",
    "        for m in auc:\n",
    "            auc_6.append(m)\n",
    "            gin_6.append(2*m - 1)\n",
    "        pr = cross_val_score(model, X_new, y, scoring = 'precision', cv = 10)\n",
    "        for m in pr:\n",
    "            pr_6.append(m)\n",
    "        rec = cross_val_score(model, X_new, y, scoring = 'recall', cv = 10)\n",
    "        for m in rec:\n",
    "            rec_6.append(m)\n",
    "    ACC6[str(comb[i])] = sum(acc_6)/len(acc_6)\n",
    "    ACCstd6[str(comb[i])] = np.std(acc_6)\n",
    "    AUC6[str(comb[i])] = sum(auc_6)/len(auc_6)\n",
    "    AUCstd6[str(comb[i])] = np.std(auc_6)\n",
    "    GIN6[str(comb[i])] = sum(gin_6)/len(gin_6)\n",
    "    GINstd6[str(comb[i])] = np.std(gin_6)\n",
    "    PR6[str(comb[i])] = sum(pr_6)/len(pr_6)\n",
    "    PRstd6[str(comb[i])] = np.std(pr_7)\n",
    "    REC6[str(comb[i])] = sum(rec_6)/len(rec_6)\n",
    "    RECstd6[str(comb[i])] = np.std(rec_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after evaluation has been completed, user can take a look into each dictionary and make a decision, given multiple evaluation metrics, what feature (sub)set(s) perfromed best\n",
    "#apart from competing cases, in which all (or most) evaluation metrics from one feature (sub)set are not consistently stronger than from another, the following tests should be performed for separate evaluation metrics\n",
    "#non-parametric Kruskal-Wallis test (scipy.stats.mstats.kruskalwallis or scipy.stats.kruskal) and non-parametric pairwise Mann-Whitney U test (scipy.stats.mannwhitneyu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature (sub)set with five strongest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to the search of a feature (sub)set containing seven features, so different parts of code will not be commented\n",
    "comb = list(itertools.combinations(features, 5))\n",
    "ACC5 = {}\n",
    "AUC5 = {}\n",
    "GIN5 = {}\n",
    "PR5 = {}\n",
    "REC5 = {}\n",
    "ACCstd5 = {}\n",
    "AUCstd5 = {}\n",
    "GINstd5 = {}\n",
    "PRstd5 = {}\n",
    "RECstd5 = {}\n",
    "learner = BernoulliNB(parameters)\n",
    "smtnc = RandomOverSampler(sampling_strategy = 0.5, random_state = 0)\n",
    "for i in range(len(comb)):\n",
    "    acc_5 = []\n",
    "    auc_5 = []\n",
    "    pr_5 = []\n",
    "    rec_5 = []\n",
    "    gin_5 = []\n",
    "    cols = []\n",
    "    for j in range(len(comb[i])):\n",
    "        for o in range(len(comb[i][j])):\n",
    "            cols.append(comb[i][j][o])\n",
    "    X_new = X[cols]\n",
    "    model = Pipeline([('smtnc', smtnc), ('clf', learner)])\n",
    "    for l in range(loops):\n",
    "        acc = cross_val_score(model, X_new, y, scoring = 'accuracy', cv = 10)\n",
    "        for m in acc:\n",
    "            acc_5.append(m)\n",
    "        auc = cross_val_score(model, X_new, y, scoring = 'roc_auc', cv = 10)\n",
    "        for m in auc:\n",
    "            auc_5.append(m)\n",
    "            gin_5.append(2*m - 1)\n",
    "        pr = cross_val_score(model, X_new, y, scoring = 'precision', cv = 10)\n",
    "        for m in pr:\n",
    "            pr_5.append(m)\n",
    "        rec = cross_val_score(model, X_new, y, scoring = 'recall', cv = 10)\n",
    "        for m in rec:\n",
    "            rec_5.append(m)\n",
    "    ACC5[str(comb[i])] = sum(acc_5)/len(acc_5)\n",
    "    ACCstd5[str(comb[i])] = np.std(acc_5)\n",
    "    AUC5[str(comb[i])] = sum(auc_5)/len(auc_5)\n",
    "    AUCstd5[str(comb[i])] = np.std(auc_5)\n",
    "    GIN5[str(comb[i])] = sum(gin_5)/len(gin_5)\n",
    "    GINstd5[str(comb[i])] = np.std(gin_5)\n",
    "    PR5[str(comb[i])] = sum(pr_5)/len(pr_5)\n",
    "    PRstd5[str(comb[i])] = np.std(pr_5)\n",
    "    REC5[str(comb[i])] = sum(rec_5)/len(rec_5)\n",
    "    RECstd5[str(comb[i])] = np.std(rec_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after evaluation has been completed, user can take a look into each dictionary and make a decision, given multiple evaluation metrics, what feature (sub)set(s) perfromed best\n",
    "#apart from competing cases, in which all (or most) evaluation metrics from one feature (sub)set are not consistently stronger than from another, the following tests should be performed for separate evaluation metrics\n",
    "#non-parametric Kruskal-Wallis test (scipy.stats.mstats.kruskalwallis or scipy.stats.kruskal) and non-parametric pairwise Mann-Whitney U test (scipy.stats.mannwhitneyu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature (sub)set with four strongest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to the search of a feature (sub)set containing seven features, so different parts of code will not be commented\n",
    "comb = list(itertools.combinations(features, 4))\n",
    "ACC4 = {}\n",
    "AUC4 = {}\n",
    "GIN4 = {}\n",
    "PR4 = {}\n",
    "REC4 = {}\n",
    "ACCstd4 = {}\n",
    "AUCstd4 = {}\n",
    "GINstd4 = {}\n",
    "PRstd4 = {}\n",
    "RECstd4 = {}\n",
    "learner = BernoulliNB(parameters)\n",
    "smtnc = RandomOverSampler(sampling_strategy = 0.5, random_state = 0)\n",
    "for i in range(len(comb)):\n",
    "    acc_4 = []\n",
    "    auc_4 = []\n",
    "    pr_4 = []\n",
    "    rec_4 = []\n",
    "    gin_4 = []\n",
    "    cols = []\n",
    "    for j in range(len(comb[i])):\n",
    "        for o in range(len(comb[i][j])):\n",
    "            cols.append(comb[i][j][o])\n",
    "    X_new = X[cols]\n",
    "    model = Pipeline([('smtnc', smtnc), ('clf', learner)])\n",
    "    for l in range(loops):\n",
    "        acc = cross_val_score(model, X_new, y, scoring = 'accuracy', cv = 10)\n",
    "        for m in acc:\n",
    "            acc_4.append(m)\n",
    "        auc = cross_val_score(model, X_new, y, scoring = 'roc_auc', cv = 10)\n",
    "        for m in auc:\n",
    "            auc_4.append(m)\n",
    "            gin_4.append(2*m - 1)\n",
    "        pr = cross_val_score(model, X_new, y, scoring = 'precision', cv = 10)\n",
    "        for m in pr:\n",
    "            pr_4.append(m)\n",
    "        rec = cross_val_score(model, X_new, y, scoring = 'recall', cv = 10)\n",
    "        for m in rec:\n",
    "            rec_4.append(m)\n",
    "    ACC4[str(comb[i])] = sum(acc_4)/len(acc_4)\n",
    "    ACCstd4[str(comb[i])] = np.std(acc_4)\n",
    "    AUC4[str(comb[i])] = sum(auc_4)/len(auc_4)\n",
    "    AUCstd4[str(comb[i])] = np.std(auc_4)\n",
    "    GIN4[str(comb[i])] = sum(gin_4)/len(gin_4)\n",
    "    GINstd4[str(comb[i])] = np.std(gin_4)\n",
    "    PR4[str(comb[i])] = sum(pr_4)/len(pr_4)\n",
    "    PRstd4[str(comb[i])] = np.std(pr_4)\n",
    "    REC4[str(comb[i])] = sum(rec_4)/len(rec_4)\n",
    "    RECstd4[str(comb[i])] = np.std(rec_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after evaluation has been completed, user can take a look into each dictionary and make a decision, given multiple evaluation metrics, what feature (sub)set(s) perfromed best\n",
    "#apart from competing cases, in which all (or most) evaluation metrics from one feature (sub)set are not consistently stronger than from another, the following tests should be performed for separate evaluation metrics\n",
    "#non-parametric Kruskal-Wallis test (scipy.stats.mstats.kruskalwallis or scipy.stats.kruskal) and non-parametric pairwise Mann-Whitney U test (scipy.stats.mannwhitneyu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature (sub)set with three strongest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to the search of a feature (sub)set containing seven features, so different parts of code will not be commented\n",
    "comb = list(itertools.combinations(features, 3))\n",
    "ACC3 = {}\n",
    "AUC3 = {}\n",
    "GIN3 = {}\n",
    "PR3 = {}\n",
    "REC3 = {}\n",
    "ACCstd3 = {}\n",
    "AUCstd3 = {}\n",
    "GINstd3 = {}\n",
    "PRstd3 = {}\n",
    "RECstd3 = {}\n",
    "learner = BernoulliNB(parameters)\n",
    "smtnc = RandomOverSampler(sampling_strategy = 0.5, random_state = 0)\n",
    "for i in range(len(comb)):\n",
    "    acc_3 = []\n",
    "    auc_3 = []\n",
    "    pr_3 = []\n",
    "    rec_3 = []\n",
    "    gin_3 = []\n",
    "    cols = []\n",
    "    for j in range(len(comb[i])):\n",
    "        for o in range(len(comb[i][j])):\n",
    "            cols.append(comb[i][j][o])\n",
    "    X_new = X[cols]\n",
    "    model = Pipeline([('smtnc', smtnc), ('clf', learner)])\n",
    "    for l in range(loops):\n",
    "        acc = cross_val_score(model, X_new, y, scoring = 'accuracy', cv = 10)\n",
    "        for m in acc:\n",
    "            acc_3.append(m)\n",
    "        auc = cross_val_score(model, X_new, y, scoring = 'roc_auc', cv = 10)\n",
    "        for m in auc:\n",
    "            auc_3.append(m)\n",
    "            gin_3.append(2*m - 1)\n",
    "        pr = cross_val_score(model, X_new, y, scoring = 'precision', cv = 10)\n",
    "        for m in pr:\n",
    "            pr_3.append(m)\n",
    "        rec = cross_val_score(model, X_new, y, scoring = 'recall', cv = 10)\n",
    "        for m in rec:\n",
    "            rec_3.append(m)\n",
    "    ACC3[str(comb[i])] = sum(acc_3)/len(acc_3)\n",
    "    ACCstd3[str(comb[i])] = np.std(acc_3)\n",
    "    AUC3[str(comb[i])] = sum(auc_3)/len(auc_3)\n",
    "    AUCstd3[str(comb[i])] = np.std(auc_3)\n",
    "    GIN3[str(comb[i])] = sum(gin_3)/len(gin_3)\n",
    "    GINstd3[str(comb[i])] = np.std(gin_3)\n",
    "    PR3[str(comb[i])] = sum(pr_3)/len(pr_3)\n",
    "    PRstd3[str(comb[i])] = np.std(pr_3)\n",
    "    REC3[str(comb[i])] = sum(rec_3)/len(rec_3)\n",
    "    RECstd3[str(comb[i])] = np.std(rec_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after evaluation has been completed, user can take a look into each dictionary and make a decision, given multiple evaluation metrics, what feature (sub)set(s) perfromed best\n",
    "#apart from competing cases, in which all (or most) evaluation metrics from one feature (sub)set are not consistently stronger than from another, the following tests should be performed for separate evaluation metrics\n",
    "#non-parametric Kruskal-Wallis test (scipy.stats.mstats.kruskalwallis or scipy.stats.kruskal) and non-parametric pairwise Mann-Whitney U test (scipy.stats.mannwhitneyu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature (sub)set with two strongest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to the search of a feature (sub)set containing seven features, so different parts of code will not be commented\n",
    "comb = list(itertools.combinations(features, 2))\n",
    "ACC2 = {}\n",
    "AUC2 = {}\n",
    "GIN2 = {}\n",
    "PR2 = {}\n",
    "REC2 = {}\n",
    "ACCstd2 = {}\n",
    "AUCstd2 = {}\n",
    "GINstd2 = {}\n",
    "PRstd2 = {}\n",
    "RECstd2 = {}\n",
    "learner = BernoulliNB(parameters)\n",
    "smtnc = RandomOverSampler(sampling_strategy = 0.5, random_state = 0)\n",
    "for i in range(len(comb)):\n",
    "    acc_2 = []\n",
    "    auc_2 = []\n",
    "    pr_2 = []\n",
    "    rec_2 = []\n",
    "    gin_2 = []\n",
    "    cols = []\n",
    "    for j in range(len(comb[i])):\n",
    "        for o in range(len(comb[i][j])):\n",
    "            cols.append(comb[i][j][o])\n",
    "    X_new = X[cols]\n",
    "    model = Pipeline([('smtnc', smtnc), ('clf', learner)])\n",
    "    for l in range(loops):\n",
    "        acc = cross_val_score(model, X_new, y, scoring = 'accuracy', cv = 10)\n",
    "        for m in acc:\n",
    "            acc_2.append(m)\n",
    "        auc = cross_val_score(model, X_new, y, scoring = 'roc_auc', cv = 10)\n",
    "        for m in auc:\n",
    "            auc_2.append(m)\n",
    "            gin_2.append(2*m - 1)\n",
    "        pr = cross_val_score(model, X_new, y, scoring = 'precision', cv = 10)\n",
    "        for m in pr:\n",
    "            pr_2.append(m)\n",
    "        rec = cross_val_score(model, X_new, y, scoring = 'recall', cv = 10)\n",
    "        for m in rec:\n",
    "            rec_2.append(m)\n",
    "    ACC2[str(comb[i])] = sum(acc_2)/len(acc_2)\n",
    "    ACCstd2[str(comb[i])] = np.std(acc_2)\n",
    "    AUC2[str(comb[i])] = sum(auc_2)/len(auc_2)\n",
    "    AUCstd2[str(comb[i])] = np.std(auc_2)\n",
    "    GIN2[str(comb[i])] = sum(gin_2)/len(gin_2)\n",
    "    GINstd2[str(comb[i])] = np.std(gin_2)\n",
    "    PR2[str(comb[i])] = sum(pr_2)/len(pr_2)\n",
    "    PRstd2[str(comb[i])] = np.std(pr_2)\n",
    "    REC2[str(comb[i])] = sum(rec_2)/len(rec_2)\n",
    "    RECstd2[str(comb[i])] = np.std(rec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after evaluation has been completed, user can take a look into each dictionary and make a decision, given multiple evaluation metrics, what feature (sub)set(s) perfromed best\n",
    "#apart from competing cases, in which all (or most) evaluation metrics from one feature (sub)set are not consistently stronger than from another, the following tests should be performed for separate evaluation metrics\n",
    "#non-parametric Kruskal-Wallis test (scipy.stats.mstats.kruskalwallis or scipy.stats.kruskal) and non-parametric pairwise Mann-Whitney U test (scipy.stats.mannwhitneyu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
