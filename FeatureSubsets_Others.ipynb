{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, cross_val_predict\n",
    "import itertools\n",
    "import operator\n",
    "from sklearn import metrics\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTENC, SMOTE, RandomOverSampler #using these oversampling methods, since the original data set is imbalanced\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data (file 'example_data.csv' with a separator ';')\n",
    "#columns in this example: 'sensor role', 'supply chain leg', 'distance to previous generated event (leg_rel)',\n",
    "#'distance to expected next event (leg_abs)', 'atmosphere temperature at current location (temp_p)', 'setpoint deviation (spd)',\n",
    "#'slope of two recent measurements', 'average deviation before a triggered alarm within one hour',\n",
    "#'average deviation after a triggered alarm within one next hour' (estimated with the help of random forest regressor),\n",
    "#'lower threshold', 'higher threshold', 'alarm label' (target feature)\n",
    "df = pd.read_csv('example_data.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescaling ambient temperature in terms of setpoint deviation units\n",
    "#df.iloc[i, 10] corresponds to higher threshold, df.iloc[i, 9] - to lower, and df.iloc[i, 4] contains the initial unscaled ambient temperature value\n",
    "for i in range(len(df)):\n",
    "    middle = (df.iloc[i, 10] - df.iloc[i, 9])/2 + df.iloc[i, 9]\n",
    "    span = (df.iloc[i, 10] - df.iloc[i, 9])/2\n",
    "    if df.iloc[i, 4] > middle:\n",
    "        df.iloc[i, 4] = (df.iloc[i, 4] - middle)/span\n",
    "    elif df.iloc[i, 4] < middle:\n",
    "        df.iloc[i, 4] = (df.iloc[i, 4] - middle)/span\n",
    "    else:\n",
    "        df.iloc[i, 4] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using only needed columns for predictor features and a target feature\n",
    "y = df['label']\n",
    "X = df[['sen_role', 'sc_leg', 'leg_rel', 'leg_abs', 'temp_p', 'spd', 'slope', 'db_1h', 'da_1h']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting categorical variables into dummy variables and creating additional columns for this purpose\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting columns that do not contain additional information\n",
    "X = X.drop(['sen_role_AMB', 'sc_leg_e'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking whether conversion went without errors\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying predictor features from which feature (sub)sets will be built and compared\n",
    "features = [['leg_rel'], ['leg_abs'], ['temp_p'], ['spd'], ['slope'], ['db_1h'], ['da_1h'], ['sen_role_REG'],\n",
    "            ['sc_leg_f', 'sc_leg_h', 'sc_leg_p']] #supply chain leg is represented by three dummy features that cannot be separated from each other, therefore they are lited as one element in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loops = loops #instead of 'loops' specify the value for a number of evaluations based on which comparison of scores will undertaken\n",
    "#(the higher the value, the more reliable the comparison; minimum recommended value: 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature (sub)set with seven strongest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb = list(itertools.combinations(features, 7)) #iterator for feature combinations\n",
    "\n",
    "#creating dictionaries for different evaluation metrics that will collect feature (sub)sets and their scores\n",
    "#evaluation metrics include accuracy (ACC), area under curve (AUC), Gini coefficient (GIN), precision (PR), recall (REC) and their standard deviations\n",
    "ACC7 = {}\n",
    "AUC7 = {}\n",
    "GIN7 = {}\n",
    "PR7 = {}\n",
    "REC7 = {}\n",
    "ACCstd7 = {}\n",
    "AUCstd7 = {}\n",
    "GINstd7 = {}\n",
    "PRstd7 = {}\n",
    "RECstd7 = {}\n",
    "\n",
    "#specifying what features are categorical (for a correct oversampling strategy later)\n",
    "categorical = ['sen_role_REG', 'sc_leg_f', 'sc_leg_h', 'sc_leg_p']\n",
    "\n",
    "learner = ExampleClassifier(optimal_parameters) #this line should be replaced by the initialization of any of the classifiers compared and optimal parameters are specified based on grid hyperparameter selection procedure\n",
    "\n",
    "#loops for each feature (sub)set (irrespective of the order of features in a (sub)set)\n",
    "for i in range(len(comb)):\n",
    "    acc_7 = []\n",
    "    auc_7 = []\n",
    "    pr_7 = []\n",
    "    rec_7 = []\n",
    "    gin_7 = []\n",
    "    cols = []\n",
    "    for j in range(len(comb[i])):\n",
    "        for o in range(len(comb[i][j])):\n",
    "            cols.append(comb[i][j][o])\n",
    "    X_new = X[cols]\n",
    "    col_list = list(X_new.columns)\n",
    "    cat_feat = []\n",
    "    for k in categorical:\n",
    "        try:\n",
    "            cat_feat.append(col_list.index(k))\n",
    "        except:\n",
    "            pass\n",
    "    if len(cat_feat) < len(col_list) and len(cat_feat) != 0:\n",
    "        smtnc = SMOTENC(categorical_features = cat_feat, sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "    elif len(cat_feat) == len(col_list):\n",
    "        smtnc = RandomOverSampler(sampling_strategy = 0.5, random_state = 0)\n",
    "    elif len(cat_feat) == 0:\n",
    "        smtnc = SMOTE(sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "    else:\n",
    "        print('Error')\n",
    "    model = Pipeline([('smtnc', smtnc), ('clf', learner)])\n",
    "    \n",
    "    #repeating evaluations 'loops' number of times for a comparison later\n",
    "    for l in range(loops):\n",
    "        acc = cross_val_score(model, X_new, y, scoring = 'accuracy', cv = 10)\n",
    "        for m in acc:\n",
    "            acc_7.append(m)\n",
    "        auc = cross_val_score(model, X_new, y, scoring = 'roc_auc', cv = 10)\n",
    "        for m in auc:\n",
    "            auc_7.append(m)\n",
    "            gin_7.append(2*m - 1)\n",
    "        pr = cross_val_score(model, X_new, y, scoring = 'precision', cv = 10)\n",
    "        for m in pr:\n",
    "            pr_7.append(m)\n",
    "        rec = cross_val_score(model, X_new, y, scoring = 'recall', cv = 10)\n",
    "        for m in rec:\n",
    "            rec_7.append(m)\n",
    "    \n",
    "    #appening each of the dictionaries (key: name of a feature (sub)set, value: average evaluation metric value)\n",
    "    ACC7[str(comb[i])] = sum(acc_7)/len(acc_7)\n",
    "    ACCstd7[str(comb[i])] = np.std(acc_7)\n",
    "    AUC7[str(comb[i])] = sum(auc_7)/len(auc_7)\n",
    "    AUCstd7[str(comb[i])] = np.std(auc_7)\n",
    "    GIN7[str(comb[i])] = sum(gin_7)/len(gin_7)\n",
    "    GINstd7[str(comb[i])] = np.std(gin_7)\n",
    "    PR7[str(comb[i])] = sum(pr_7)/len(pr_7)\n",
    "    PRstd7[str(comb[i])] = np.std(pr_7)\n",
    "    REC7[str(comb[i])] = sum(rec_7)/len(rec_7)\n",
    "    RECstd7[str(comb[i])] = np.std(rec_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after evaluation has been completed, user can take a look into each dictionary and make a decision, given multiple evaluation metrics, what feature (sub)set(s) perfromed best\n",
    "#apart from competing cases, in which all (or most) evaluation metrics from one feature (sub)set are not consistently stronger than from another, the following tests should be performed for separate evaluation metrics\n",
    "#non-parametric Kruskal-Wallis test (scipy.stats.mstats.kruskalwallis or scipy.stats.kruskal) and non-parametric pairwise Mann-Whitney U test (scipy.stats.mannwhitneyu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature (sub)set with six strongest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to the search of a feature (sub)set containing seven features, so different parts of code will not be commented\n",
    "comb = list(itertools.combinations(features, 6))\n",
    "ACC6 = {}\n",
    "AUC6 = {}\n",
    "GIN6 = {}\n",
    "PR6 = {}\n",
    "REC6 = {}\n",
    "ACCstd6 = {}\n",
    "AUCstd6 = {}\n",
    "GINstd6 = {}\n",
    "PRstd6 = {}\n",
    "RECstd6 = {}\n",
    "categorical = ['sen_role_REG', 'sc_leg_f', 'sc_leg_h', 'sc_leg_p']\n",
    "learner = ExampleClassifier(optimal_parameters)\n",
    "for i in range(len(comb)):\n",
    "    acc_6 = []\n",
    "    auc_6 = []\n",
    "    pr_6 = []\n",
    "    rec_6 = []\n",
    "    gin_6 = []\n",
    "    cols = []\n",
    "    for j in range(len(comb[i])):\n",
    "        for o in range(len(comb[i][j])):\n",
    "            cols.append(comb[i][j][o])\n",
    "    X_new = X[cols]\n",
    "    col_list = list(X_new.columns)\n",
    "    cat_feat = []\n",
    "    for k in categorical:\n",
    "        try:\n",
    "            cat_feat.append(col_list.index(k))\n",
    "        except:\n",
    "            pass\n",
    "    if len(cat_feat) < len(col_list) and len(cat_feat) != 0:\n",
    "        smtnc = SMOTENC(categorical_features = cat_feat, sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "    elif len(cat_feat) == len(col_list):\n",
    "        smtnc = RandomOverSampler(sampling_strategy = 0.5, random_state = 0)\n",
    "    elif len(cat_feat) == 0:\n",
    "        smtnc = SMOTE(sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "    else:\n",
    "        print('Error')\n",
    "    model = Pipeline([('smtnc', smtnc), ('clf', learner)])\n",
    "    for l in range(loops):\n",
    "        acc = cross_val_score(model, X_new, y, scoring = 'accuracy', cv = 10)\n",
    "        for m in acc:\n",
    "            acc_6.append(m)\n",
    "        auc = cross_val_score(model, X_new, y, scoring = 'roc_auc', cv = 10)\n",
    "        for m in auc:\n",
    "            auc_6.append(m)\n",
    "            gin_6.append(2*m - 1)\n",
    "        pr = cross_val_score(model, X_new, y, scoring = 'precision', cv = 10)\n",
    "        for m in pr:\n",
    "            pr_6.append(m)\n",
    "        rec = cross_val_score(model, X_new, y, scoring = 'recall', cv = 10)\n",
    "        for m in rec:\n",
    "            rec_6.append(m)\n",
    "    ACC6[str(comb[i])] = sum(acc_6)/len(acc_6)\n",
    "    ACCstd6[str(comb[i])] = np.std(acc_6)\n",
    "    AUC6[str(comb[i])] = sum(auc_6)/len(auc_6)\n",
    "    AUCstd6[str(comb[i])] = np.std(auc_6)\n",
    "    GIN6[str(comb[i])] = sum(gin_6)/len(gin_6)\n",
    "    GINstd6[str(comb[i])] = np.std(gin_6)\n",
    "    PR6[str(comb[i])] = sum(pr_6)/len(pr_6)\n",
    "    PRstd6[str(comb[i])] = np.std(pr_7)\n",
    "    REC6[str(comb[i])] = sum(rec_6)/len(rec_6)\n",
    "    RECstd6[str(comb[i])] = np.std(rec_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after evaluation has been completed, user can take a look into each dictionary and make a decision, given multiple evaluation metrics, what feature (sub)set(s) perfromed best\n",
    "#apart from competing cases, in which all (or most) evaluation metrics from one feature (sub)set are not consistently stronger than from another, the following tests should be performed for separate evaluation metrics\n",
    "#non-parametric Kruskal-Wallis test (scipy.stats.mstats.kruskalwallis or scipy.stats.kruskal) and non-parametric pairwise Mann-Whitney U test (scipy.stats.mannwhitneyu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature (sub)set with five strongest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to the search of a feature (sub)set containing seven features, so different parts of code will not be commented\n",
    "comb = list(itertools.combinations(features, 5))\n",
    "ACC5 = {}\n",
    "AUC5 = {}\n",
    "GIN5 = {}\n",
    "PR5 = {}\n",
    "REC5 = {}\n",
    "ACCstd5 = {}\n",
    "AUCstd5 = {}\n",
    "GINstd5 = {}\n",
    "PRstd5 = {}\n",
    "RECstd5 = {}\n",
    "categorical = ['sen_role_REG', 'sc_leg_f', 'sc_leg_h', 'sc_leg_p']\n",
    "learner = ExampleClassifier(optimal_parameters)\n",
    "for i in range(len(comb)):\n",
    "    acc_5 = []\n",
    "    auc_5 = []\n",
    "    pr_5 = []\n",
    "    rec_5 = []\n",
    "    gin_5 = []\n",
    "    cols = []\n",
    "    for j in range(len(comb[i])):\n",
    "        for o in range(len(comb[i][j])):\n",
    "            cols.append(comb[i][j][o])\n",
    "    X_new = X[cols]\n",
    "    col_list = list(X_new.columns)\n",
    "    cat_feat = []\n",
    "    for k in categorical:\n",
    "        try:\n",
    "            cat_feat.append(col_list.index(k))\n",
    "        except:\n",
    "            pass\n",
    "    if len(cat_feat) < len(col_list) and len(cat_feat) != 0:\n",
    "        smtnc = SMOTENC(categorical_features = cat_feat, sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "    elif len(cat_feat) == len(col_list):\n",
    "        smtnc = RandomOverSampler(sampling_strategy = 0.5, random_state = 0)\n",
    "    elif len(cat_feat) == 0:\n",
    "        smtnc = SMOTE(sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "    else:\n",
    "        print('Error')\n",
    "    model = Pipeline([('smtnc', smtnc), ('clf', learner)])\n",
    "    for l in range(loops):\n",
    "        acc = cross_val_score(model, X_new, y, scoring = 'accuracy', cv = 10)\n",
    "        for m in acc:\n",
    "            acc_5.append(m)\n",
    "        auc = cross_val_score(model, X_new, y, scoring = 'roc_auc', cv = 10)\n",
    "        for m in auc:\n",
    "            auc_5.append(m)\n",
    "            gin_5.append(2*m - 1)\n",
    "        pr = cross_val_score(model, X_new, y, scoring = 'precision', cv = 10)\n",
    "        for m in pr:\n",
    "            pr_5.append(m)\n",
    "        rec = cross_val_score(model, X_new, y, scoring = 'recall', cv = 10)\n",
    "        for m in rec:\n",
    "            rec_5.append(m)\n",
    "    ACC5[str(comb[i])] = sum(acc_5)/len(acc_5)\n",
    "    ACCstd5[str(comb[i])] = np.std(acc_5)\n",
    "    AUC5[str(comb[i])] = sum(auc_5)/len(auc_5)\n",
    "    AUCstd5[str(comb[i])] = np.std(auc_5)\n",
    "    GIN5[str(comb[i])] = sum(gin_5)/len(gin_5)\n",
    "    GINstd5[str(comb[i])] = np.std(gin_5)\n",
    "    PR5[str(comb[i])] = sum(pr_5)/len(pr_5)\n",
    "    PRstd5[str(comb[i])] = np.std(pr_5)\n",
    "    REC5[str(comb[i])] = sum(rec_5)/len(rec_5)\n",
    "    RECstd5[str(comb[i])] = np.std(rec_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after evaluation has been completed, user can take a look into each dictionary and make a decision, given multiple evaluation metrics, what feature (sub)set(s) perfromed best\n",
    "#apart from competing cases, in which all (or most) evaluation metrics from one feature (sub)set are not consistently stronger than from another, the following tests should be performed for separate evaluation metrics\n",
    "#non-parametric Kruskal-Wallis test (scipy.stats.mstats.kruskalwallis or scipy.stats.kruskal) and non-parametric pairwise Mann-Whitney U test (scipy.stats.mannwhitneyu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature (sub)set with four strongest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to the search of a feature (sub)set containing seven features, so different parts of code will not be commented\n",
    "comb = list(itertools.combinations(features, 4))\n",
    "ACC4 = {}\n",
    "AUC4 = {}\n",
    "GIN4 = {}\n",
    "PR4 = {}\n",
    "REC4 = {}\n",
    "ACCstd4 = {}\n",
    "AUCstd4 = {}\n",
    "GINstd4 = {}\n",
    "PRstd4 = {}\n",
    "RECstd4 = {}\n",
    "categorical = ['sen_role_REG', 'sc_leg_f', 'sc_leg_h', 'sc_leg_p']\n",
    "learner = ExampleClassifier(optimal_parameters)\n",
    "for i in range(len(comb)):\n",
    "    acc_4 = []\n",
    "    auc_4 = []\n",
    "    pr_4 = []\n",
    "    rec_4 = []\n",
    "    gin_4 = []\n",
    "    cols = []\n",
    "    for j in range(len(comb[i])):\n",
    "        for o in range(len(comb[i][j])):\n",
    "            cols.append(comb[i][j][o])\n",
    "    X_new = X[cols]\n",
    "    col_list = list(X_new.columns)\n",
    "    cat_feat = []\n",
    "    for k in categorical:\n",
    "        try:\n",
    "            cat_feat.append(col_list.index(k))\n",
    "        except:\n",
    "            pass\n",
    "    if len(cat_feat) < len(col_list) and len(cat_feat) != 0:\n",
    "        smtnc = SMOTENC(categorical_features = cat_feat, sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "    elif len(cat_feat) == len(col_list):\n",
    "        smtnc = RandomOverSampler(sampling_strategy = 0.5, random_state = 0)\n",
    "    elif len(cat_feat) == 0:\n",
    "        smtnc = SMOTE(sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "    else:\n",
    "        print('Error')\n",
    "    model = Pipeline([('smtnc', smtnc), ('clf', learner)])\n",
    "    for l in range(loops):\n",
    "        acc = cross_val_score(model, X_new, y, scoring = 'accuracy', cv = 10)\n",
    "        for m in acc:\n",
    "            acc_4.append(m)\n",
    "        auc = cross_val_score(model, X_new, y, scoring = 'roc_auc', cv = 10)\n",
    "        for m in auc:\n",
    "            auc_4.append(m)\n",
    "            gin_4.append(2*m - 1)\n",
    "        pr = cross_val_score(model, X_new, y, scoring = 'precision', cv = 10)\n",
    "        for m in pr:\n",
    "            pr_4.append(m)\n",
    "        rec = cross_val_score(model, X_new, y, scoring = 'recall', cv = 10)\n",
    "        for m in rec:\n",
    "            rec_4.append(m)\n",
    "    ACC4[str(comb[i])] = sum(acc_4)/len(acc_4)\n",
    "    ACCstd4[str(comb[i])] = np.std(acc_4)\n",
    "    AUC4[str(comb[i])] = sum(auc_4)/len(auc_4)\n",
    "    AUCstd4[str(comb[i])] = np.std(auc_4)\n",
    "    GIN4[str(comb[i])] = sum(gin_4)/len(gin_4)\n",
    "    GINstd4[str(comb[i])] = np.std(gin_4)\n",
    "    PR4[str(comb[i])] = sum(pr_4)/len(pr_4)\n",
    "    PRstd4[str(comb[i])] = np.std(pr_4)\n",
    "    REC4[str(comb[i])] = sum(rec_4)/len(rec_4)\n",
    "    RECstd4[str(comb[i])] = np.std(rec_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after evaluation has been completed, user can take a look into each dictionary and make a decision, given multiple evaluation metrics, what feature (sub)set(s) perfromed best\n",
    "#apart from competing cases, in which all (or most) evaluation metrics from one feature (sub)set are not consistently stronger than from another, the following tests should be performed for separate evaluation metrics\n",
    "#non-parametric Kruskal-Wallis test (scipy.stats.mstats.kruskalwallis or scipy.stats.kruskal) and non-parametric pairwise Mann-Whitney U test (scipy.stats.mannwhitneyu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature (sub)set with three strongest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to the search of a feature (sub)set containing seven features, so different parts of code will not be commented\n",
    "comb = list(itertools.combinations(features, 3))\n",
    "ACC3 = {}\n",
    "AUC3 = {}\n",
    "GIN3 = {}\n",
    "PR3 = {}\n",
    "REC3 = {}\n",
    "ACCstd3 = {}\n",
    "AUCstd3 = {}\n",
    "GINstd3 = {}\n",
    "PRstd3 = {}\n",
    "RECstd3 = {}\n",
    "categorical = ['sen_role_REG', 'sc_leg_f', 'sc_leg_h', 'sc_leg_p']\n",
    "learner = ExampleClassifier(optimal_parameters)\n",
    "for i in range(len(comb)):\n",
    "    acc_3 = []\n",
    "    auc_3 = []\n",
    "    pr_3 = []\n",
    "    rec_3 = []\n",
    "    gin_3 = []\n",
    "    cols = []\n",
    "    for j in range(len(comb[i])):\n",
    "        for o in range(len(comb[i][j])):\n",
    "            cols.append(comb[i][j][o])\n",
    "    X_new = X[cols]\n",
    "    col_list = list(X_new.columns)\n",
    "    cat_feat = []\n",
    "    for k in categorical:\n",
    "        try:\n",
    "            cat_feat.append(col_list.index(k))\n",
    "        except:\n",
    "            pass\n",
    "    if len(cat_feat) < len(col_list) and len(cat_feat) != 0:\n",
    "        smtnc = SMOTENC(categorical_features = cat_feat, sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "    elif len(cat_feat) == len(col_list):\n",
    "        smtnc = RandomOverSampler(sampling_strategy = 0.5, random_state = 0)\n",
    "    elif len(cat_feat) == 0:\n",
    "        smtnc = SMOTE(sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "    else:\n",
    "        print('Error')\n",
    "    model = Pipeline([('smtnc', smtnc), ('clf', learner)])\n",
    "    for l in range(loops):\n",
    "        acc = cross_val_score(model, X_new, y, scoring = 'accuracy', cv = 10)\n",
    "        for m in acc:\n",
    "            acc_3.append(m)\n",
    "        auc = cross_val_score(model, X_new, y, scoring = 'roc_auc', cv = 10)\n",
    "        for m in auc:\n",
    "            auc_3.append(m)\n",
    "            gin_3.append(2*m - 1)\n",
    "        pr = cross_val_score(model, X_new, y, scoring = 'precision', cv = 10)\n",
    "        for m in pr:\n",
    "            pr_3.append(m)\n",
    "        rec = cross_val_score(model, X_new, y, scoring = 'recall', cv = 10)\n",
    "        for m in rec:\n",
    "            rec_3.append(m)\n",
    "    ACC3[str(comb[i])] = sum(acc_3)/len(acc_3)\n",
    "    ACCstd3[str(comb[i])] = np.std(acc_3)\n",
    "    AUC3[str(comb[i])] = sum(auc_3)/len(auc_3)\n",
    "    AUCstd3[str(comb[i])] = np.std(auc_3)\n",
    "    GIN3[str(comb[i])] = sum(gin_3)/len(gin_3)\n",
    "    GINstd3[str(comb[i])] = np.std(gin_3)\n",
    "    PR3[str(comb[i])] = sum(pr_3)/len(pr_3)\n",
    "    PRstd3[str(comb[i])] = np.std(pr_3)\n",
    "    REC3[str(comb[i])] = sum(rec_3)/len(rec_3)\n",
    "    RECstd3[str(comb[i])] = np.std(rec_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after evaluation has been completed, user can take a look into each dictionary and make a decision, given multiple evaluation metrics, what feature (sub)set(s) perfromed best\n",
    "#apart from competing cases, in which all (or most) evaluation metrics from one feature (sub)set are not consistently stronger than from another, the following tests should be performed for separate evaluation metrics\n",
    "#non-parametric Kruskal-Wallis test (scipy.stats.mstats.kruskalwallis or scipy.stats.kruskal) and non-parametric pairwise Mann-Whitney U test (scipy.stats.mannwhitneyu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature (sub)set with two strongest features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the procedure is analogical to the search of a feature (sub)set containing seven features, so different parts of code will not be commented\n",
    "comb = list(itertools.combinations(features, 2))\n",
    "ACC2 = {}\n",
    "AUC2 = {}\n",
    "GIN2 = {}\n",
    "PR2 = {}\n",
    "REC2 = {}\n",
    "ACCstd2 = {}\n",
    "AUCstd2 = {}\n",
    "GINstd2 = {}\n",
    "PRstd2 = {}\n",
    "RECstd2 = {}\n",
    "categorical = ['sen_role_REG', 'sc_leg_f', 'sc_leg_h', 'sc_leg_p']\n",
    "learner = ExampleClassifier(optimal_parameters)\n",
    "for i in range(len(comb)):\n",
    "    acc_2 = []\n",
    "    auc_2 = []\n",
    "    pr_2 = []\n",
    "    rec_2 = []\n",
    "    gin_2 = []\n",
    "    cols = []\n",
    "    for j in range(len(comb[i])):\n",
    "        for o in range(len(comb[i][j])):\n",
    "            cols.append(comb[i][j][o])\n",
    "    X_new = X[cols]\n",
    "    col_list = list(X_new.columns)\n",
    "    cat_feat = []\n",
    "    for k in categorical:\n",
    "        try:\n",
    "            cat_feat.append(col_list.index(k))\n",
    "        except:\n",
    "            pass\n",
    "    if len(cat_feat) < len(col_list) and len(cat_feat) != 0:\n",
    "        smtnc = SMOTENC(categorical_features = cat_feat, sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "    elif len(cat_feat) == len(col_list):\n",
    "        smtnc = RandomOverSampler(sampling_strategy = 0.5, random_state = 0)\n",
    "    elif len(cat_feat) == 0:\n",
    "        smtnc = SMOTE(sampling_strategy = 0.5, random_state = 0, n_jobs = -1)\n",
    "    else:\n",
    "        print('Error')\n",
    "    model = Pipeline([('smtnc', smtnc), ('clf', learner)])\n",
    "    for l in range(loops):\n",
    "        acc = cross_val_score(model, X_new, y, scoring = 'accuracy', cv = 10)\n",
    "        for m in acc:\n",
    "            acc_2.append(m)\n",
    "        auc = cross_val_score(model, X_new, y, scoring = 'roc_auc', cv = 10)\n",
    "        for m in auc:\n",
    "            auc_2.append(m)\n",
    "            gin_2.append(2*m - 1)\n",
    "        pr = cross_val_score(model, X_new, y, scoring = 'precision', cv = 10)\n",
    "        for m in pr:\n",
    "            pr_2.append(m)\n",
    "        rec = cross_val_score(model, X_new, y, scoring = 'recall', cv = 10)\n",
    "        for m in rec:\n",
    "            rec_2.append(m)\n",
    "    ACC2[str(comb[i])] = sum(acc_2)/len(acc_2)\n",
    "    ACCstd2[str(comb[i])] = np.std(acc_2)\n",
    "    AUC2[str(comb[i])] = sum(auc_2)/len(auc_2)\n",
    "    AUCstd2[str(comb[i])] = np.std(auc_2)\n",
    "    GIN2[str(comb[i])] = sum(gin_2)/len(gin_2)\n",
    "    GINstd2[str(comb[i])] = np.std(gin_2)\n",
    "    PR2[str(comb[i])] = sum(pr_2)/len(pr_2)\n",
    "    PRstd2[str(comb[i])] = np.std(pr_2)\n",
    "    REC2[str(comb[i])] = sum(rec_2)/len(rec_2)\n",
    "    RECstd2[str(comb[i])] = np.std(rec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after evaluation has been completed, user can take a look into each dictionary and make a decision, given multiple evaluation metrics, what feature (sub)set(s) perfromed best\n",
    "#apart from competing cases, in which all (or most) evaluation metrics from one feature (sub)set are not consistently stronger than from another, the following tests should be performed for separate evaluation metrics\n",
    "#non-parametric Kruskal-Wallis test (scipy.stats.mstats.kruskalwallis or scipy.stats.kruskal) and non-parametric pairwise Mann-Whitney U test (scipy.stats.mannwhitneyu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
